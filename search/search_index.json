{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#maxtext","title":"MaxText","text":""},{"location":"#overview","title":"Overview","text":"<p>MaxText is a high performance, highly scalable, open-source LLM written in pure Python/Jax and targeting Google Cloud TPUs and GPUs for training and inference. MaxText achieves high MFUs and scales from single host to very large clusters while staying simple and \"optimization-free\" thanks to the power of Jax and the XLA compiler.</p> <p>MaxText aims to be a launching off point for ambitious LLM projects both in research and production. We encourage users to start by experimenting with MaxText out of the box and then fork and modify MaxText to meet their needs.</p> <p>We have used MaxText to demonstrate high-performance, well-converging training in int8 and scale training to ~51K chips.</p> <p>Key supported features: * TPUs and GPUs (in preview) * Training and Inference (in preview) * Models: Llama2, Mistral and Gemma</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Getting Started</li> <li>Runtime Performance Results</li> <li>Comparison To Alternatives</li> <li>Development</li> <li>Features and Diagnostics</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>For your first time running MaxText, we provide specific instructions.</p> <p>MaxText supports training and inference of various open models. Follow user guides in the getting started folder to know more.</p> <p>Some extra helpful guides: * Gemma: a family of open-weights Large Language Model (LLM) by Google DeepMind, based on Gemini research and technology. You can run decode and finetuning using these instructions. * Llama2: a family of open-weights Large Language Model (LLM) by Meta. You can run decode and finetuning using these instructions. * Mixtral: a family of open-weights sparse mixture-of-experts (MoE) model by Mistral AI. You can run decode and finetuning using these instructions</p> <p>In addition to the getting started guides, there are always other MaxText capabilities that are being constantly being added! The full suite of end-to-end tests is in end_to_end. We run them with a nightly cadence. They can be a good source for understanding MaxText Alternatively you can see the continuous unit tests which are run almost continuously.</p>"},{"location":"#runtime-performance-results","title":"Runtime Performance Results","text":"<p>More details on reproducing these results can be found in MaxText/configs/README.md.</p>"},{"location":"#tpu-v5p","title":"TPU v5p","text":"No. of params Accelerator Type TFLOP/chip/sec Model flops utilization (MFU) 32B v5p-128 3.28e+02 71.47% 64B v5p-128 3.23e+02 70.31% 128B v5p-256 3.15e+02 68.68% 128B v5p-512 3.15e+02 68.53% 256B v5p-1024 3.16e+02 68.82% 512B v5p-1024 2.94e+02 63.99% 1024B v5p-2048 2.49e+02 64.05% 1024B v5p-4096 2.97e+02 64.80% 1160B v5p-7680 2.95e+02 64.27% 1160B v5p-12288 3.04e+02 66.23%"},{"location":"#tpu-v5e","title":"TPU v5e","text":"<p>For 16B, 32B, 64B, and 128B models. See full run configs in MaxText/configs/v5e/ as <code>16b.sh</code>, <code>32b.sh</code>, <code>64b.sh</code>, <code>128b.sh</code>.</p> Hardware 16B TFLOP/sec/chip 16B MFU 32B TFLOP/sec/chip 32B MFU 64B TFLOP/sec/chip 64B MFU 128B TFLOP/sec/chip 128B MFU 1x v5e-256 120 61.10% 132 66.86% 118 59.90% 110 56.06% 2x v5e-256 117 59.37% 128 64.81% 112 56.66% 110 55.82% 4x v5e-256 117 59.14% 126 64.10% 110 55.85% 108 54.93% 8x v5e-256 115 58.27% 125 63.67% 108 54.96% 104 52.93% 16x v5e-256 111 56.56% 123 62.26% 105 53.29% 100 50.86% 32x v5e-256 108 54.65% 119 60.40% 99 50.18% 91 46.25%"},{"location":"#comparison-to-alternatives","title":"Comparison to Alternatives","text":"<p>MaxText is heavily inspired by MinGPT/NanoGPT, elegant standalone GPT implementations written in PyTorch and targeting Nvidia GPUs. MaxText is more complex, supporting more industry standard models and scaling to tens of thousands of chips. Ultimately MaxText has an MFU more than three times the 17% reported most recently with that codebase, is massively scalable and implements a key-value cache for efficient auto-regressive decoding.</p> <p>MaxText is more similar to Nvidia/Megatron-LM, a very well tuned LLM implementation targeting Nvidia GPUs. The two implementations achieve comparable MFUs. The difference in the codebases highlights the different programming strategies. MaxText is pure Python, relying heavily on the XLA compiler to achieve high performance. By contrast, Megatron-LM is a mix of Python and CUDA, relying on well-optimized CUDA kernels to achieve high performance.</p> <p>MaxText is also comparable to Pax. Like Pax, MaxText provides high-performance and scalable implementations of LLMs in Jax. Pax focuses on enabling powerful configuration parameters, enabling developers to change the model by editing config parameters. By contrast, MaxText is a simple, concrete implementation of various LLMs that encourages users to extend by forking and directly editing the source code.</p>"},{"location":"#features-and-diagnostics","title":"Features and Diagnostics","text":""},{"location":"#collect-stack-traces","title":"Collect Stack Traces","text":"<p>When running a Single Program, Multiple Data (SPMD) job on accelerators, the overall process can hang if there is any error or any VM hangs/crashes for some reason. In this scenario, capturing stack traces will help to identify and troubleshoot the issues for the jobs running on TPU VMs.</p> <p>The following configurations will help to debug a fault or when a program is stuck or hung somewhere by collecting stack traces. Change the parameter values accordingly in <code>MaxText/configs/base.yml</code>: 1. Set <code>collect_stack_trace: True</code> to enable collection of stack traces on faults or when the program is hung. This setting will periodically dump the traces for the program to help in debugging. To disable this, set <code>collect_stack_trace: False</code>. 2. Set <code>stack_trace_to_cloud: False</code> to display stack traces on console. <code>stack_trace_to_cloud: True</code> will create a temporary file in <code>/tmp/debugging</code> in the TPUs to store the stack traces. There is an agent running on TPU VMs that will periodically upload the traces from the temporary directory to cloud logging in the gcp project. You can view the traces in Logs Explorer on Cloud Logging using the following query:</p> <pre><code>logName=\"projects/&lt;project_name&gt;/logs/tpu.googleapis.com%2Fruntime_monitor\"\njsonPayload.verb=\"stacktraceanalyzer\"\n</code></pre> <ol> <li><code>stack_trace_interval_seconds</code> signifies the duration in seconds between each stack trace collection event. Setting <code>stack_trace_interval_seconds: 600</code> will collect the stack traces every 600 seconds (10 minutes).</li> </ol> <p>Here is the related PyPI package: https://pypi.org/project/cloud-tpu-diagnostics.</p>"},{"location":"#ahead-of-time-compilation-aot","title":"Ahead of Time Compilation (AOT)","text":"<p>To compile your training run ahead of time, we provide a tool <code>train_compile.py</code>. This tool allows you to compile the main <code>train_step</code> in <code>train.py</code> for target hardware (e.g. a large number of v5e devices) without using the full cluster.</p>"},{"location":"#tpu-support","title":"TPU Support","text":"<p>You may use only a CPU or a single VM from a different family to pre-compile for a TPU cluster. This compilation helps with two main goals:</p> <ul> <li> <p>It will flag any out of memory (OOM) information, such as when the <code>per_device_batch_size</code> is set too high, with an identical OOM stack trace as if it was compiled on the target hardware.</p> </li> <li> <p>The ahead of time compilation can be saved and then loaded for fast startup and restart times on the target hardware.</p> </li> </ul> <p>The tool <code>train_compile.py</code> is tightly linked to <code>train.py</code> and uses the same configuration file <code>configs/base.yml</code>. Although you don't need to run on a TPU, you do need to install <code>jax[tpu]</code> in addition to other dependencies, so we recommend running <code>setup.sh</code> to install these if you have not already done so.</p>"},{"location":"#example-aot-1-compile-ahead-of-time-basics","title":"Example AOT 1: Compile ahead of time basics","text":"<p>After installing the dependencies listed above, you are ready to compile ahead of time:</p> <pre><code># Run the below on a single machine, e.g. a CPU\npython3 MaxText/train_compile.py MaxText/configs/base.yml compile_topology=v5e-256 compile_topology_num_slices=2 \\\nglobal_parameter_scale=16 per_device_batch_size=4\n</code></pre> <p>This will compile a 16B parameter MaxText model on 2 v5e pods.</p>"},{"location":"#example-aot-2-save-compiled-function-then-load-and-run-it","title":"Example AOT 2: Save compiled function, then load and run it","text":"<p>Here is an example that saves then loads the compiled <code>train_step</code>, starting with the save:</p> <p>Step 1: Run AOT and save compiled function</p> <pre><code># Run the below on a single machine, e.g. a CPU\nexport LIBTPU_INIT_ARGS=\"--xla_enable_async_all_gather=true\"\npython3 MaxText/train_compile.py MaxText/configs/base.yml compile_topology=v5e-256 \\\ncompile_topology_num_slices=2 \\\ncompiled_trainstep_file=my_compiled_train.pickle global_parameter_scale=16 \\\nper_device_batch_size=4 steps=10000 learning_rate=1e-3\n</code></pre> <p>Step 2: Run train.py and load the compiled function</p> <p>To load the compiled train_step, you just need to pass <code>compiled_trainstep_file=my_compiled_train.pickle</code> into <code>train.py</code>:</p> <pre><code># Run the below on each host of the target hardware, e.g. each host on 2 slices of v5e-256\nexport LIBTPU_INIT_ARGS=\"--xla_enable_async_all_gather=true\"\npython3 MaxText/train.py MaxText/configs/base.yml run_name=example_load_compile \\\ncompiled_trainstep_file=my_compiled_train.pickle \\\nglobal_parameter_scale=16  per_device_batch_size=4 steps=10000 learning_rate=1e-3 \\\nbase_output_directory=gs://my-output-bucket dataset_path=gs://my-dataset-bucket\n</code></pre> <p>In the save step of example 2 above we included exporting the compiler flag <code>LIBTPU_INIT_ARGS</code> and <code>learning_rate</code> because those affect the compiled object <code>my_compiled_train.pickle.</code> The sizes of the model (e.g. <code>global_parameter_scale</code>, <code>max_sequence_length</code> and <code>per_device_batch</code>) are fixed when you initially compile via <code>compile_train.py</code>, you will see a size error if you try to run the saved compiled object with different sizes than you compiled with. However a subtle note is that the learning rate schedule is also fixed when you run <code>compile_train</code> - which is determined by both <code>steps</code> and <code>learning_rate</code>. The optimizer parameters such as  <code>adam_b1</code> are passed only as shaped objects to the compiler - thus their real values are determined when you run <code>train.py</code>, not during the compilation. If you do pass in different shapes (e.g. <code>per_device_batch</code>), you will get a clear error message reporting that the compiled signature has different expected shapes than what was input. If you attempt to run on different hardware than the compilation targets requested via <code>compile_topology</code>, you will get an error saying there is a failure to map the devices from the compiled to your real devices. Using different XLA flags or a LIBTPU than what was compiled will probably run silently with the environment you compiled in without error. However there is no guaranteed behavior in this case; you should run in the same environment you compiled in.</p>"},{"location":"#gpu-support","title":"GPU Support","text":"<p>Ahead-of-time compilation is also supported for GPUs with some differences from TPUs: </p> <ol> <li> <p>GPU does not support compilation across hardware: A GPU host is still required to run AoT compilation, but a single GPU host can compile a program for a larger cluster of the same hardware.</p> </li> <li> <p>For A3 Cloud GPUs, the maximum \"slice\" size is a single host, and the <code>compile_topology_num_slices</code> parameter represents the number of A3 machines to precompile for.</p> </li> </ol>"},{"location":"#example","title":"Example","text":"<p>This example illustrates the flags to use for a multihost GPU compilation targeting a cluster of 4 A3 hosts:</p> <p>Step 1: Run AOT and save compiled function</p> <pre><code># Run the below on a single A3 machine\nexport XLA_FLAGS=\"--xla_gpu_enable_async_collectives=true\"\npython3 MaxText/train_compile.py MaxText/configs/base.yml compile_topology=a3 \\\ncompile_topology_num_slices=4 \\\ncompiled_trainstep_file=my_compiled_train.pickle global_parameter_scale=16 \\\nattention=dot_product per_device_batch_size=4 steps=10000 learning_rate=1e-3\n</code></pre> <p>Step 2: Run train.py and load the compiled function</p> <p>To load the compiled train_step, you just need to pass <code>compiled_trainstep_file=my_compiled_train.pickle</code> into <code>train.py</code>:</p> <pre><code># Run the below on each of the 4 target A3 hosts.\nexport XLA_FLAGS=\"--xla_gpu_enable_async_collectives=true\"\npython3 MaxText/train.py MaxText/configs/base.yml run_name=example_load_compile \\\ncompiled_trainstep_file=my_compiled_train.pickle \\\nattention=dot_product global_parameter_scale=16  per_device_batch_size=4 steps=10000 learning_rate=1e-3 \\\nbase_output_directory=gs://my-output-bucket dataset_path=gs://my-dataset-bucket\n</code></pre> <p>As in the TPU case, note that the compilation environment must match the execution environment, in this case by setting the same <code>XLA_FLAGS</code>.</p>"},{"location":"#automatically-upload-logs-to-vertex-tensorboard","title":"Automatically Upload Logs to Vertex Tensorboard","text":"<p>MaxText supports automatic upload of logs collected in a directory to a Tensorboard instance in Vertex AI. Follow user guide to know more.</p>"},{"location":"about/","title":"What is Maxtext?","text":"<p>MaxText is a Google initiated open source project for high performance, highly scalable, open-source LLM written in pure Python/JAX and targeting Google Cloud TPUs and GPUs for training and inference.</p> <p>MaxText achieves very high MFUs (Model Flop Utilization) and scales from single host to very large clusters while staying simple and \"optimization-free\".</p> <p>MaxText additionally provides an highly optimized reference implementations for popular Open Source models like:</p> <ul> <li>Llama 2, 3 and 3.1</li> <li>Mistral and Mixtral</li> <li>Gemma and Gemma2</li> <li>GPT</li> </ul> <p>These reference implementations support pre-training and full fine tuning.  Maxtext also allows you to create various sized models for benchmarking purposes.</p> <p>The key value proposition of using MaxText for pre-training or full fine tuning is:</p> <ul> <li>Very high performance of average of 50% MFU</li> <li>Open code base - Code base can be found at the following github location.</li> <li>Easy to understand: MaxText is purely written in JAX and Python, which makes it accessible to ML developers interested in inspecting the implementation or stepping through it. It is written at the block-by-block level, with code for Embeddings, Attention, Normalization etc. Different Attention mechanisms like MQA and GQA are all present. For quantization, it uses the JAX AQT library. The implementation is suitable for both GPUs and TPUs.</li> </ul> <p>MaxText aims to be a launching off point for ambitious LLM projects both in research and production. We encourage users to start by experimenting with MaxText out of the box and then fork and modify MaxText to meet their needs.</p> <p>!!! note</p> <pre><code>Maxtext today only supports Pre-training and Full Fine Tuning of the models. It does not support PEFT/LoRA, Supervised Fine Tuning or RLHF\n</code></pre>"},{"location":"about/#who-is-the-target-user-of-maxtext","title":"Who is the target user of Maxtext?","text":"<ul> <li>Any individual or a company that is interested in forking maxtext and seeing it as a reference implementation of a high performance Large Language Models and wants to build their own LLMs on TPU and GPU.</li> <li>Any individual or a company that is interested in performing a pre-training or Full Fine Tuning of the supported open source models, can use Maxtext as a blackbox to perform full fine tuning. Maxtext attains an extremely high MFU, resulting in large savings in training costs.</li> </ul>"},{"location":"data_loading/","title":"Data Loading","text":"<p>Maxtext supports input data pipelines in the following ways: Tf.data* Grain Hugging Face Datasets</p> <p>*Tf.data is the most performant way of loading large scale datasets.  </p> <p>You can read more about the pipelines in .</p>"},{"location":"getting_started/Data_Input_Perf/","title":"Data Input Perf","text":""},{"location":"getting_started/Data_Input_Perf/#performance-of-data-input-pipeline","title":"Performance of Data Input Pipeline","text":"<ul> <li>Overview of supported data input pipelines: https://github.com/google/maxtext/blob/main/getting_started/Data_Input_Pipeline.md</li> <li>Perf data intepretation: for all three data pipelines, there are data prefetch running in parallel with computation. The goal is to hide data loading behind computation. As long as data loading step time &lt; training computation step time, the data pipeline perf is considered sufficient. </li> </ul>"},{"location":"getting_started/Data_Input_Perf/#methods","title":"Methods","text":"<ul> <li>The following results are measured by standalone_dataloader.py, which performs data loading without computation.</li> <li>c4 data of different formats in GCS bucket are used. For Grain pipeline only, the GCS bucket is mounted to a local path via GCSFUSE (script)</li> <li>The GCS bucket is multi-region (US) and the VMs that read data can be in different regions in the US.</li> </ul>"},{"location":"getting_started/Data_Input_Perf/#huggingface-pipeline","title":"HuggingFace pipeline","text":"<p>The following data are collected using c4 data in Parquet format.</p> Pipeline seq_len VM type per_host_batch # of host # of batch first step (s) total time (s) HuggingFace 2048 TPU v4-8 32 (per_device=8) 1 1000 6 72 HuggingFace 2048 TPU v4-128 32 (per_device=8) 16 1000 6 72"},{"location":"getting_started/Data_Input_Perf/#grain-pipeline","title":"Grain pipeline","text":"<p>The following data are collected using c4 data in ArrayRecord format.</p> Pipeline seq_len VM type per_host_batch # of host # of batch worker first step (s) total time (s) Grain 2048 TPU v4-8 32 (per_device=8) 1 1000 1 7 1200 Grain 2048 TPU v4-8 32 (per_device=8) 1 1000 2 7 355 Grain 2048 TPU v4-8 32 (per_device=8) 1 1000 4 8 280 Grain 2048 TPU v4-8 32 (per_device=8) 1 1000 8 15 367 Grain 2048 TPU v4-128 32 (per_device=8) 16 1000 1 7 691 Grain 2048 TPU v4-128 32 (per_device=8) 16 1000 2 7 335 Grain 2048 TPU v4-128 32 (per_device=8) 16 1000 4 8 154 Grain 2048 TPU v4-128 32 (per_device=8) 16 1000 8 11 120"},{"location":"getting_started/Data_Input_Perf/#tfds-pipeline","title":"TFDS pipeline","text":"<p>The following data are collected using c4 data in TFRecord format.</p> Pipeline seq_len VM type per_host_batch # of host # of batch first step (s) total time (s) TFDS 2048 TPU v4-8 32 (per_device=8) 1 1000 2 17 TFDS 2048 TPU v4-128 32 (per_device=8) 16 1000 3 18"},{"location":"getting_started/Data_Input_Pipeline/","title":"Data Input Pipeline","text":""},{"location":"getting_started/Data_Input_Pipeline/#data-input-pipeline","title":"Data Input Pipeline","text":"<p>Currently MaxText has three data input pipelines:</p> Pipeline Dataset formats Features Limitations HuggingFace datasets in HuggingFace Hublocal/Cloud Storage datasets in json, parquet, arrow, csv, txt conveniencemultiple formats limit scalability using HuggingFace Hubnon-deterministic with preemption(deterministic without preemption) Grain ArrayRecord, available through Tensorflow Datasets fully deterministic, regardless of preemption only supports random access datasets TFDS TFRecord, available through Tensorflow Datasets only supports TFRecordsnon-deterministic with preemption(deterministic without preemption)"},{"location":"getting_started/Data_Input_Pipeline/#performance","title":"Performance","text":"<ul> <li>Perf data for all 3 input pipeline: https://github.com/google/maxtext/blob/main/getting_started/Data_Input_Perf.md</li> </ul>"},{"location":"getting_started/Data_Input_Pipeline/#multihost-dataloading-best-practice","title":"Multihost dataloading best practice","text":"<p>In multihost environment, if use an input pipeline that reads data sequentially (HuggingFace or TFDS), the most performant way is to have each data file only accessed by one host, and each host access a subset of data files (shuffle is within the subset of files). This requires (# of data files) to be multiples of (# of hosts loading data). We recommand users to reshard the dataset or use a subset of hosts to load data by setting expansion_factor_real_data (only available for some topologies, will error out otherwise). In MaxText, since the goal is to demonstrate the most performant experience, the behaviors for different data pipelines are:</p>"},{"location":"getting_started/Data_Input_Pipeline/#huggingface-pipeline-in-multihost","title":"HuggingFace pipeline in multihost","text":"<ul> <li>When (# of data files) &gt;= (# of hosts loading data), assign files to each host as evenly as possible, some host may ended up with 1 file more than the others. When some hosts run out of data, they will produce empty padding batches, so that you are able to utilize the data from the hosts that still have data. But in this stage, training/eval will be less effective, and you will see a decrease in total_weights and slower change in loss. If all hosts run out of data before the step number you set, you will see 0 total_weights and 0 loss. The training/eval will run until the steps/eval_steps set in the config. Note that even each host are assigned the same number of data files, due to the different example count in each data file, and example packing, you will still have different number of batches on each host near the end of the epoch.</li> <li>When (# of data files) &lt; (# of hosts loading data), files are read sequentially with multiple hosts accessing each file, perf can degrade quickly as # of host increases.</li> </ul>"},{"location":"getting_started/Data_Input_Pipeline/#tfds-pipeline-in-multihost","title":"TFDS pipeline in multihost","text":"<ul> <li>When (# of data files) &gt;= (# of hosts loading data), assign equal number of files to each host. The remainning files are skipped. Train/eval will hang if steps/eval_steps are not met but some hosts run out of data. Please set steps/eval_steps accordingly.</li> <li>When (# of data files) &lt; (# of hosts loading data), files are read sequentially with multiple hosts accessing each file, perf can degrade quickly as # of host increases.</li> </ul>"},{"location":"getting_started/Data_Input_Pipeline/#grain-pipeline-in-multihost","title":"Grain pipeline in multihost","text":"<ul> <li>Perf not affected by (# of data files) vs (# of hosts loading data). Data are shuffled globally. Because grain uses a data format (ArrayRecord) that supports random access by index. Even with multiple hosts accessing the same file, they are accessing different indices and and won't have the issue seen with sequential reading.</li> <li>At the end of the dataset, you may still have some hosts runing out of indices and hang, Please set steps/eval_steps accordingly.</li> </ul>"},{"location":"getting_started/Data_Input_Pipeline/#huggingface-pipeline","title":"HuggingFace pipeline","text":"<p>The HuggingFace pipeline supports streaming directly from HuggingFace Hub, or from GCS bucket in HuggingFace supported formats (parquet, json, etc.). This is through the HuggingFace <code>datasets.load_dataset</code> API with <code>streaming=True</code>, which take in <code>hf_*</code> parameters.</p>"},{"location":"getting_started/Data_Input_Pipeline/#example-config-for-streaming-from-huggingface-hub-no-download-needed","title":"Example config for streaming from HuggingFace Hub (no download needed):","text":"<pre><code>dataset_type: hf\nhf_path: 'allenai/c4'  # for using https://huggingface.co/datasets/allenai/c4\nhf_data_dir: 'en'\nhf_train_files: ''\n# set eval_interval &gt; 0 to use the specified eval dataset, otherwise, only metrics on the train set will be calculated.\neval_interval: 10000\nhf_eval_split: 'validation'\nhf_eval_files: ''\n# for HF pipeline, tokenizer_path can be a path in HuggingFace Hub, \n# or a local path containing tokenizer in a format supported by transformers.AutoTokenizer\ntokenizer_path: 'google-t5/t5-large'  # for using https://huggingface.co/google-t5/t5-large\nhf_access_token: ''  # provide token if using gated dataset or tokenizer\n</code></pre>"},{"location":"getting_started/Data_Input_Pipeline/#example-config-for-streaming-from-downloaded-data-in-a-gcs-bucket","title":"Example config for streaming from downloaded data in a GCS bucket:","text":"<pre><code>dataset_type: hf\nhf_path: 'parquet'  # or json, arrow, etc.\nhf_data_dir: ''\nhf_train_files: 'gs://&lt;bucket&gt;/&lt;folder&gt;/*-train-*.parquet'   # match the train files\n# set eval_interval &gt; 0 to use the specified eval dataset. Otherwise, only metrics on the train set will be calculated.\neval_interval: 10000\nhf_eval_split: ''\nhf_eval_files: 'gs://&lt;bucket&gt;/&lt;folder&gt;/*-validation-*.parquet'  # match the val files\n# for HF pipeline, tokenizer_path can be a path in HuggingFace Hub, \n# or a local path containing tokenizer in a format supported by transformers.AutoTokenizer\ntokenizer_path: 'google-t5/t5-large'  # for using https://huggingface.co/google-t5/t5-large\n</code></pre>"},{"location":"getting_started/Data_Input_Pipeline/#limitations-recommendations","title":"Limitations &amp; Recommendations","text":"<ol> <li>Streaming data directly from HuggingFace Hub may be impacted by the traffic of the server. During peak hours you may encounter \"504 Server Error: Gateway Time-out\". It's recommended to download the HuggingFace dataset to a GCS bucket or disk for the most stable experience.</li> <li>Streaming data directly from HuggingFace Hub works in multihost settings with a small number of hosts. We have encountered \"read time out\" error with host number &gt; 16.</li> <li>Only supports epoch=1 at this moment.</li> </ol>"},{"location":"getting_started/Data_Input_Pipeline/#grain-pipeline-for-determinism","title":"Grain pipeline - for determinism","text":""},{"location":"getting_started/Data_Input_Pipeline/#why-do-we-need-determinism-for-data-input-pipeline","title":"Why do we need determinism for data input pipeline?","text":"<p>Determinism in a data input pipeline means that the same input data always results in the same sequence of batches at each step. This is typically achieved by setting a fixed shuffle seed during pipeline initialization. In an ideal scenario, where training runs uninterrupted, this determinism is straightforward (deterministic without preemption). However, real-world distributed training environments often face preemptions due to maintenance, hardware failures, or resource constraints.  When a preempted training run resumes, the data input pipeline is re-initialized. If the same shuffle seed is used, the pipeline restarts from the beginning, potentially re-training the model on initial data. Conversely, a new seed produces a different batch sequence, making it difficult to track which data has been seen and how often each example is used for training. This lack of control can impact model performance and reproducibility.</p>"},{"location":"getting_started/Data_Input_Pipeline/#how-does-grain-achieve-determinism","title":"How does Grain achieve determinism","text":"<p>Grain ensures determinism in data input pipelines by saving the pipeline's state, including dataset metadata and processed data indices, within a small JSON file in checkpoints. When a training run is resumed with the same dataset and shuffle seed, Grain restores the pipeline's exact state from the checkpoint. This enables fully deterministic, reproducible training that is resilient to disruptions.</p>"},{"location":"getting_started/Data_Input_Pipeline/#cases-where-determinism-is-crucial","title":"Cases where determinism is crucial","text":"<ul> <li>Model sensitive to repetition. When models are sensitive to the frequency with which they encounter specific examples, precise control over the order and repetition of data during training is essential.</li> <li>Convergence comparison. In sensitive convergence experiments like testing quantization techniques, maintaining identical data batches between runs (e.g., quantized vs. unquantized) is essential for comparison. Determinism ensures consistency even when the runs are long and undergo saving/resuming at different steps.</li> <li>Debug training anomalies. When troubleshooting training spikes or anomalies, the ability to replay the exact data sequence helps distinguish between bad data batches and underlying hardware or software issues.</li> </ul>"},{"location":"getting_started/Data_Input_Pipeline/#global-shuffle-in-grain","title":"Global shuffle in Grain","text":"<p>In HF or TFDS data pipeline, global shuffle is performed by a shuffle buffer with limited size. Grain performs global shuffle of the indices in the beginning of each epoch and then reads the elements according to the random order. We have found this to be generally fast enough, even when using hard drives and distributed file systems.</p>"},{"location":"getting_started/Data_Input_Pipeline/#using-grain","title":"Using Grain","text":"<ol> <li>Dataset needs to be in a format that supports random access. The default format is ArrayRecord. For converting a dataset into ArrayRecord, see instructions. Additionally, other random accessible data sources can be supported via a custom data source class (docs).</li> <li>ArrayRecord dataset, when hosted on GCS bucket, can only be read through Cloud Storage FUSE. The installation of Cloud Storage FUSE is included in setup.sh. User then needs to mount the GCS bucket to a local path for each worker, using the script setup_gcsfuse.sh. The script configs some parameters for the mount.</li> </ol> <pre><code>bash setup_gcsfuse.sh DATASET_GCS_BUCKET=$BUCKET_NAME MOUNT_PATH=$MOUNT_PATH\n</code></pre> <ol> <li>Set <code>dataset_type=grain</code> and set <code>grain_train_files</code> to match the ArrayRecord files via a local path since the bucket has been mounted.</li> <li>Tune <code>grain_worker_count</code> for performance. This parameter controls the number of child process used by Grain (more details in behind_the_scene, code). If you use a large number of workers, please check your config for gcsfuse in setup_gcsfuse.sh to avoid gcsfuse throttling.</li> <li>Example command:</li> </ol> <pre><code>bash setup_gcsfuse.sh \\\nDATASET_GCS_BUCKET=maxtext-dataset \\\nMOUNT_PATH=/tmp/gcsfuse &amp;&amp; \\\npython3 MaxText/train.py MaxText/configs/base.yml \\\nrun_name=&lt;RUN_NAME&gt; base_output_directory=gs://&lt;MY_BUCKET&gt;  \\\ndataset_type=grain \\\ngrain_train_files=/tmp/gcsfuse/array-record/c4/en/3.0.1/c4-train.array_record* \\\ngrain_worker_count=2\n</code></pre> <ol> <li>Using validation set for eval When setting eval_interval &gt; 0, eval will be run with a specified eval dataset. Example config:</li> </ol> <pre><code>eval_interval: 10000\ngrain_eval_files: '/tmp/gcsfuse/array-record/c4/en/3.0.1/c4-validation.array_record*'\n</code></pre>"},{"location":"getting_started/Data_Input_Pipeline/#tfds-pipeline","title":"TFDS pipeline","text":"<ol> <li>Download the Allenai c4 dataset in TFRecord format to a GCS bucket (will cost about $100, details)</li> </ol> <pre><code>bash download_dataset.sh {GCS_PROJECT} {GCS_BUCKET_NAME}\n</code></pre> <ol> <li>Use the following config:</li> </ol> <pre><code>dataset_type: tfds\ndataset_name: 'c4/en:3.0.1'\n# set eval_interval &gt; 0 to use the specified eval dataset. Otherwise, only metrics on the train set will be calculated.\neval_interval: 10000\neval_dataset_name: 'c4/en:3.0.1'\neval_split: 'validation'\n# TFDS input pipeline only supports tokenizer in spm format\ntokenizer_path: \"assets/tokenizer.llama2\"\n</code></pre>"},{"location":"getting_started/First_run/","title":"Getting Started","text":"<p>We recommend starting with a single host first and then moving to multihost.</p>"},{"location":"getting_started/First_run/#getting-started-cloud-storage-and-configure","title":"Getting Started: Cloud Storage and Configure","text":"<ol> <li> <p>Create a gcs buckets in your project for storing logs and checkpoints. To run maxtext the TPU/GPU VMs must have permission to read/write the gcs bucket. These permissions are granted by service account roles, such as the <code>STORAGE ADMIN</code> role.</p> </li> <li> <p>MaxText reads a yaml file for configuration. We also recommend reviewing the configurable options in <code>configs/base.yml</code>, this config includes a decoder-only model of ~1B parameters. The configurable options can be overwritten from command lines. For instance you may change the <code>steps</code> or <code>log_period</code> by either modifying <code>configs/base.yml</code> or by passing in <code>steps</code> and <code>log_period</code> as additional args to the <code>train.py</code> call. <code>base_output_directory</code> should be set to a folder in the bucket you just created.</p> </li> </ol>"},{"location":"getting_started/First_run/#getting-started-local-development-for-single-host","title":"Getting Started: Local Development for single host","text":""},{"location":"getting_started/First_run/#running-on-cloud-tpus","title":"Running on Cloud TPUs","text":"<p>Local development is a convenient way to run MaxText on a single host. It doesn't scale to multiple hosts.</p> <ol> <li>Create and SSH to the single-host VM of your choice. We recommend a <code>v4-8</code>.</li> <li>Clone MaxText onto that TPUVM.</li> <li>Within the root directory of that <code>git</code> repo, install dependencies and pre-commit hook by running:</li> </ol> <pre><code>bash setup.sh\npre-commit install\n</code></pre> <ol> <li>After installation completes, run training with the command on synthetic data:</li> </ol> <pre><code>python3 MaxText/train.py MaxText/configs/base.yml \\\n  run_name=$YOUR_JOB_NAME \\\n  base_output_directory=gs://&lt;my-bucket&gt; \\\n  dataset_type=synthetic \\\n  steps=10\n</code></pre> <p>Next, you can try training on a HugginFace dataset, see Data Input Pipeline for data input options.</p> <ol> <li>If you want to decode, you can decode as follows.</li> </ol> <pre><code>python3 MaxText/decode.py MaxText/configs/base.yml \\\n  run_name=$YOUR_JOB_NAME \\\n  base_output_directory=gs://&lt;my-bucket&gt; \\\n  per_device_batch_size=1\n</code></pre> <p>Be aware, these decodings will be random. To get high quality decodings you need pass in a checkpoint, typically via the <code>load_parameters_path</code> argument.</p>"},{"location":"getting_started/First_run/#running-on-nvidia-gpus","title":"Running on NVIDIA GPUs","text":"<ol> <li>Use <code>bash docker_build_dependency_image.sh DEVICE=gpu</code> can be used to build a container with the required dependencies.</li> <li>After installation is completed, run training with the command on synthetic data:</li> </ol> <pre><code>python3 MaxText/train.py MaxText/configs/base.yml \\\n  run_name=$YOUR_JOB_NAME \\\n  base_output_directory=gs://&lt;my-bucket&gt; \\\n  dataset_type=synthetic \\\n  steps=10  \n</code></pre> <ol> <li>If you want to decode, you can decode as follows.</li> </ol> <pre><code>python3 MaxText/decode.py MaxText/configs/base.yml \\\n  run_name=$YOUR_JOB_NAME \\\n  base_output_directory=gs://&lt;my-bucket&gt; \\\n  per_device_batch_size=1  \n</code></pre> <ul> <li>If you see the following error when running inside a container, set a larger <code>--shm-size</code> (e.g. <code>--shm-size=1g</code>)</li> </ul> <pre><code>Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.all_reduce' failed: external/xla/xla/service/gpu/nccl_utils.cc:297: NCCL operation ncclCommInitRank(&amp;comm, nranks, id, rank) failed: unhandled cuda error (run with NCCL_DEBUG=INFO for details); current tracing scope: all-reduce-start.2; current profiling annotation: XlaModule:#hlo_module=jit__unnamed_wrapped_function_,program_id=7#.\n</code></pre>"},{"location":"getting_started/First_run/#getting-starting-multihost-development","title":"Getting Starting: Multihost development","text":"<p>There are three patterns for running MaxText with more than one host.</p> <ol> <li>[GKE, recommended] Running Maxtext with xpk - Quick Experimentation and Production support</li> <li>[GCE] Running Maxtext with Multihost Jobs - Long Running Production Jobs with Queued Resources</li> <li>[GCE] Running Maxtext with Multihost Runner -  Fast experiments via multiple ssh connections.</li> </ol>"},{"location":"getting_started/First_run/#getting-starting-preflight-optimizations","title":"Getting Starting: Preflight Optimizations","text":"<p>Once you've gotten workloads running, there are important optimizations you might want to put on your cluster. Please check the doc PREFLIGHT.md</p>"},{"location":"getting_started/Run_Llama2/","title":"Run Llama2","text":""},{"location":"getting_started/Run_Llama2/#about-llama2","title":"About Llama2","text":"<p>MaxText supports Llama2 pretraining, finetuning and decoding for its 7B and 70B flavors. To get started on decoding and finetuning of Llama2, you will first need to download weights along with its tokenizer from Meta. </p> <p>The file test_llama2_7b.sh provides details on how to convert the PyTorch weights in orbax checkpoint format, and thereafter use it for running decoding and finetuning. test_llama2_7b.sh also shows how to run pretraining and also how to run decoding on the finetuned model checkpoint.</p>"},{"location":"getting_started/Run_Llama2/#maxtext-supports-pretraining-and-finetuning-with-high-performance","title":"MaxText supports pretraining and finetuning with high performance.","text":"<p>Model Flop utilization for training on v5e and v5p and v4 TPUs with MaxText.</p> Model v4-128 (bf16) v5p-128 (bf16) v5e-256 (bf16) Llama2-70b 57% 65% 57%"},{"location":"getting_started/Run_MaxText_via_multihost_job/","title":"Run MaxText via multihost job","text":""},{"location":"getting_started/Run_MaxText_via_multihost_job/#getting-started-multihost_jobpy-production-jobs-on-multiple-slices","title":"Getting Started: <code>multihost_job.py</code> - Production Jobs On Multiple Slices","text":"<p>The workflow using <code>multihost_job.py</code> is optimized for long running experiments, providing resiliency against hardware failure and avoiding long running ssh connections. Its latency is much higher than <code>multihost_runner.py</code> because it needs to provision new capacity each time. The <code>multihost_job.py</code> script ends once the request to create the TPUs is issued. Logs are written both to gcloud in real time and also sent to GCS at the end of the job.</p> <p>The <code>multihost_job.py</code> script:</p> <ul> <li>Copies your code to your GCS bucket</li> <li>Spins up specified TPU VM(s) via CQR</li> <li>Directs the TPU's to download then run that code. Because this logic is within the CQR's startup script, if there hardware is interrupted, the job will be rescheduled and resumed.</li> <li>Logs to gcloud, and additionally sends the logs to GCS at the job end</li> <li> <p>Delete the TPUs and QR at the end of the job.</p> </li> <li> <p>Choose a directory on your runner machine to develop and clone MaxText into. The runner machine can either be a TPUVM or not. If your runner machine is a TPUVM, it needs service account roles that grant it permission to create queued resources and has write access to GCS, such as the <code>TPU ADMIN</code> and <code>STORAGE ADMIN</code> roles. Clone MaxText, and cd into the root of the repo.</p> </li> <li> <p>Set your project, zone.     Set your gcloud config, see https://cloud.google.com/sdk/gcloud/reference/config for more.     <code>PROJECT=&lt;project&gt;</code></p> <p><code>ZONE=&lt;zone&gt;</code></p> <p><code>gcloud config set project $PROJECT gcloud config set compute/zone $ZONE</code></p> </li> <li> <p>Link to a GCS bucket.     Create a bucket if you don't already have one, see: https://cloud.google.com/storage/docs/creating-buckets for instructions to create one. Once you've identified your bucket:</p> <p><code>BUCKET_NAME=&lt;your-bucket&gt;</code></p> </li> <li> <p>Run your training job.</p> <p>*** IMPORTANT *** <code>multihost_job</code> creates a request for new capacity for each run! You cannot use this tool on existing capacity, instead we recommend <code>multihost_runner</code> for this purpose.</p> <p>Choose the number of nodes (we use 2 below, but you may customize this and other feature of your TPU(s)) <code>NODE_COUNT=2</code> <code>RUN_NAME=$YOUR_JOB_NAME # You may set this to any unique name for a fresh run. python3 multihost_job.py --NUM_SLICES=$NODE_COUNT --RUN_NAME=$RUN_NAME --BUCKET_NAME=$BUCKET_NAME --CQR_EXTRA_ARGS=\"--reserved\" --COMMAND=\"bash setup.sh &amp;&amp; python3 MaxText/train.py MaxText/configs/base.yml run_name=$RUN_NAME\"</code></p> <p>We tell <code>multihost_job</code> to target the <code>reserved</code> pool by  by including <code>--reserved</code> as extra arguments to the CQR request, but you may instead target the <code>on-demand</code> pool by removing the <code>--CQR_EXTRA_ARGS</code> flag (on-demand is default), or the pre-emptible pool with <code>--CQR_EXTRA_ARGS=\"--best-effort\"</code>, which may be necessary if your reservation is full.</p> </li> <li> <p>View the job's logs in cloud logging.</p> <p>The link to your job's cloud logging is printed at the end of <code>multihost_job</code> output. Additionally logs are saved to GCS when your job finishes, and this bucket's URL is also printed by <code>multihost_job</code>.</p> </li> </ul>"},{"location":"getting_started/Run_MaxText_via_multihost_runner/","title":"Run MaxText via multihost runner","text":""},{"location":"getting_started/Run_MaxText_via_multihost_runner/#getting-started-multihost_runnerpy-quick-experiments-on-multiple-hosts-or-multiple-slices","title":"Getting Started: <code>multihost_runner.py</code> - Quick Experiments on Multiple Hosts (or Multiple Slices)","text":"<p>This workflow using <code>multihost_runner.py</code> is optimized for quick experiments, repeatedly re-using the same TPUs. Because the <code>multihost_runner.py</code> script depends on long-lived <code>ssh</code> connections, we do not recommend it for any long-running jobs.</p> <p>We call the <code>runner</code> machine the one that <code>multihost_runner.py</code> is called from. This script will <code>ssh</code> into TPUVM <code>worker</code> machines that are found from the <code>--TPU_PREFIX</code> flag, and must be different than the runner machine. If the runner machine is a cloud VM, it must be in the same project as the workers.</p> <p>The <code>multihost_runner.py</code> script: * Distributes your code by recursively copying the current state of the chosen directory to multiple worker TPUVM. * Runs the code on the workers * Logs and monitors the processes' error statuses and brings the logs back to the runner machine.</p> <p>Although there are several steps below, most are for the initial setup. Once setup you can continually make changes to your code and re-run your code with only step 5.</p> <ol> <li> <p>Choose a directory on your runner machine to develop and clone MaxText into. The runner machine can either be a TPUVM or not, but it cannot be one of the workers. If your runner machine is a TPUVM, it needs service account roles that grant it permission to create queued resources and ssh into them, such as the <code>TPU ADMIN</code> role. Clone MaxText, and cd into the root of the repo.</p> </li> <li> <p>Set your project, zone, and ssh keys.</p> <p>Set your gcloud config, see https://cloud.google.com/sdk/gcloud/reference/config for more. <code>PROJECT=&lt;project&gt;</code> <code>ZONE=&lt;zone&gt;</code> <code>gcloud config set project $PROJECT gcloud config set compute/zone $ZONE</code></p> <p>Create ssh keys for gcloud, we recommend leaving a blank password (hit enter twice after running the below command). If you are prompted that the the file already exists you can choose not to overwrite by selecting \"n\". <code>ssh-keygen -f ~/.ssh/google_compute_engine</code></p> </li> <li> <p>Create your instances via Queued Resource (QR).     Choose names for your TPUs and QR:     <code>TPU_PREFIX=$YOUR_TPU_NAME # Use new names when you create new TPUs     QR_ID=$TPU_PREFIX # Convenient to reuse the node names, but can be different</code>     Choose the number of nodes (we use 2 below, but you may customize this and other feature of your TPU(s))     <code>NODE_COUNT=2</code>     Create a multislice environment of nodes using create queued resources     <code>gcloud alpha compute tpus queued-resources create $QR_ID --accelerator-type=v4-8 --runtime-version=tpu-ubuntu2204-base --node-count=$NODE_COUNT --node-prefix=$TPU_PREFIX  --reserved</code>     We target the <code>reserved</code> pool above, but you may instead target the <code>on-demand</code> pool by omitting this flag,     or target pre-emptible capacity with the <code>--best-effort</code> flag, which may be necessary if your reservation is full.</p> <p>You have to wait for the QR to become <code>ACTIVE</code> (as opposed to <code>ACCEPTED</code> or <code>PROVISIONING</code>) which corresponds to the worker nodes becoming <code>READY</code> (as opposed to <code>CREATING</code>). This may take a minute or two and can be checked via <code>gcloud alpha compute tpus queued-resources list --filter=$QR_ID</code> 4. Install dependencies. Install the dependencies of <code>train.py</code> on each worker using <code>multihost_runner.py</code>: <code>python3 multihost_runner.py --TPU_PREFIX=$TPU_PREFIX --COMMAND=\"bash setup.sh\"</code> If you are running the <code>multihost_runner.py</code> script from a TPUVM, you will need to set <code>--INTERNAL_IP=true</code>.</p> </li> <li> <p>Run your training job.</p> <p>Set a RUN_NAME for your job: <code>RUN_NAME=$YOUR_JOB_NAME # You may set this to any unique name for a fresh run.</code> Set config values for <code>base_output_directory</code> and <code>dataset_path</code> in <code>configs/base.yml</code> if not set already. <code>python3 multihost_runner.py --TPU_PREFIX=$TPU_PREFIX --COMMAND=\"python3 MaxText/train.py MaxText/configs/base.yml run_name=$RUN_NAME\"</code> If you are running the <code>multihost_runner.py</code> script from a TPUVM, you will need to set <code>--INTERNAL_IP=true</code>.</p> </li> <li> <p>Clean up TPUs and QR when finished.</p> <p><code>gcloud alpha compute tpus queued-resources delete $QR_ID --force --async</code></p> <p>The <code>--force</code> flag deletes both the queued resources and the TPU VMs, without it only a <code>SUSPENDED</code> queued resource whose TPUs have already been deleted can itself be deleted. We highly recommend the <code>--async</code> flag since deleting the TPUs and QR will take a minute or two.</p> </li> </ol>"},{"location":"getting_started/Run_MaxText_via_xpk/","title":"Run MaxText via xpk","text":""},{"location":"getting_started/Run_MaxText_via_xpk/#how-to-run-maxtext-with-xpk","title":"How to run MaxText with XPK?","text":"<p>This document focuses on steps required to setup XPK on TPU VM and assumes you have gone through the README to understand XPK basics.</p>"},{"location":"getting_started/Run_MaxText_via_xpk/#steps-to-setup-xpk-on-tpu-vm","title":"Steps to setup XPK on TPU VM","text":"<ul> <li> <p>Verify you have these permissions for your account or service account</p> <p>Storage Admin \\ Kubernetes Engine Admin</p> </li> <li> <p>gcloud is installed on TPUVMs using the snap distribution package. Install kubectl using snap</p> </li> </ul> <pre><code>sudo apt-get update\nsudo apt install snapd\nsudo snap install kubectl --classic\n</code></pre> <ul> <li>Install <code>gke-gcloud-auth-plugin</code></li> </ul> <pre><code>echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\n\ncurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -\n\nsudo apt update &amp;&amp; sudo apt-get install google-cloud-sdk-gke-gcloud-auth-plugin\n</code></pre> <ul> <li>Authenticate gcloud installation by running this command and following the prompt</li> </ul> <pre><code>gcloud auth login\n</code></pre> <ul> <li>Run this command to configure docker to use docker-credential-gcloud for GCR registries:</li> </ul> <pre><code>gcloud auth configure-docker\n</code></pre> <ul> <li>Test the installation by running</li> </ul> <pre><code>docker run hello-world\n</code></pre> <ul> <li>If getting a permission error, try running</li> </ul> <pre><code>sudo usermod -aG docker $USER\n</code></pre> <p>after which log out and log back in to the machine.</p>"},{"location":"getting_started/Run_MaxText_via_xpk/#build-docker-image-for-maxtext","title":"Build Docker Image for Maxtext","text":"<ol> <li> <p>Git clone maxtext locally</p> <p><code>shell git clone https://github.com/google/maxtext.git cd maxtext</code> 2. Build local Maxtext docker image</p> <p>This only needs to be rerun when you want to change your dependencies. This image may expire which would require you to rerun the below command</p> <p>```shell</p> </li> </ol>"},{"location":"getting_started/Run_MaxText_via_xpk/#default-will-pick-stable-versions-of-dependencies","title":"Default will pick stable versions of dependencies","text":"<p>bash docker_build_dependency_image.sh <code>`` 3. After building the dependency image</code>maxtext_base_image<code>, xpk can handle updates to the working directory when running</code>xpk workload create<code>and using</code>--base-docker-image`.</p> <p>See details on docker images in xpk here: https://github.com/google/xpk/blob/main/README.md#how-to-add-docker-images-to-a-xpk-workload</p> <p>Using xpk to upload image to your gcp project and run Maxtext</p> <p>```shell   gcloud config set project $PROJECT_ID   gcloud config set compute/zone $ZONE</p> <p># See instructions in README.me to create below buckets.   BASE_OUTPUT_DIR=gs://output_bucket/   DATASET_PATH=gs://dataset_bucket/</p> <p># Install xpk   pip install xpk</p> <p># Make sure you are still in the maxtext github root directory when running this command   xpk workload create \\   --cluster ${CLUSTER_NAME} \\   --base-docker-image maxtext_base_image \\   --workload ${USER}-first-job \\   --tpu-type=v5litepod-256 \\   --num-slices=1  \\   --command \"python3 MaxText/train.py MaxText/configs/base.yml base_output_directory=${BASE_OUTPUT_DIR} dataset_path=${DATASET_PATH} steps=100 per_device_batch_size=1\"   ```</p> <p>Using xpk github repo</p> <p>```shell   git clone https://github.com/google/xpk.git</p> <p># Make sure you are still in the maxtext github root directory when running this command   python3 xpk/xpk.py workload create \\   --cluster ${CLUSTER_NAME} \\   --base-docker-image maxtext_base_image \\   --workload ${USER}-first-job \\   --tpu-type=v5litepod-256 \\   --num-slices=1  \\   --command \"python3 MaxText/train.py MaxText/configs/base.yml base_output_directory=${BASE_OUTPUT_DIR} dataset_path=${DATASET_PATH} steps=100 per_device_batch_size=1\"   ```</p>"},{"location":"getting_started/Use_Vertex_AI_Tensorboard/","title":"Use Vertex AI Tensorboard","text":""},{"location":"getting_started/Use_Vertex_AI_Tensorboard/#use-vertex-ai-tensorboard","title":"Use Vertex AI Tensorboard","text":"<p>MaxText supports automatic upload of logs collected in a directory to a Tensorboard instance in Vertex AI. For more information on how MaxText supports this feature, visit cloud-accelerator-diagnostics PyPI package documentation.</p>"},{"location":"getting_started/Use_Vertex_AI_Tensorboard/#what-is-vertex-ai-tensorboard-and-vertex-ai-experiment","title":"What is Vertex AI Tensorboard and Vertex AI Experiment","text":"<p>Vertex AI Tensorboard is a fully managed and enterprise-ready version of open-source Tensorboard. To learn more about Vertex AI Tensorboard, visit this. Vertex AI Experiment is a tool that helps to track and analyze an experiment run on Vertex AI Tensorboard. To learn more about Vertex AI Experiments, visit this. </p> <p>You can use a single Vertex AI Tensorboard instance to track and compare metrics from multiple Vertex AI Experiments. While you can view metrics from multiple Vertex AI Experiments within a single Tensorboard instance, the underlying log data for each experiment remains separate.</p>"},{"location":"getting_started/Use_Vertex_AI_Tensorboard/#prerequisites","title":"Prerequisites","text":"<ul> <li>Enable Vertex AI API in your Google Cloud console.</li> <li>Assign Vertex AI User IAM role to the service account used by the TPU VMs. This is required to create and access the Vertex AI Tensorboard in Google Cloud console. If you are using XPK for MaxText, the necessary Vertex AI User IAM role will be automatically assigned to your node pools by XPK \u2013 no need to assign it manually.</li> </ul>"},{"location":"getting_started/Use_Vertex_AI_Tensorboard/#upload-logs-to-vertex-ai-tensorboard","title":"Upload Logs to Vertex AI Tensorboard","text":"<p>Scenario 1: Using XPK to run MaxText on GKE</p> <p>XPK simplifies MaxText's Vertex AI Tensorboard integration. A Vertex Tensorboard instance and Experiment are automatically created by XPK during workload scheduling. Also, XPK automatically sets the necessary environment variables, eliminating the need to manually configure this in MaxText. Set <code>use_vertex_tensorboard=False</code> to avoid setting up Vertex Tensorboard again in MaxText. This is how the configuration will look like for running MaxText via XPK:</p> <pre><code>use_vertex_tensorboard: False\nvertex_tensorboard_project: \"\"\nvertex_tensorboard_region: \"\"\n</code></pre> <p>The above configuration will upload logs in <code>config.tensorboard_dir</code> to Vertex Tensorboard instance set as an environment variable by XPK.</p> <p>Scenario 2: Running MaxText on GCE</p> <p>Set <code>use_vertex_tensorboard=True</code> to upload logs in <code>config.tensorboard_dir</code> to a Tensorboard instance in Vertex AI. You can manually create a Tensorboard instance named <code>&lt;config.vertex_tensorboard_project&gt;-tb-instance</code> and an Experiment named <code>config.run_name</code> in Vertex AI on Google Cloud console. Otherwise, MaxText will create those resources for you when <code>use_vertex_tensorboard=True</code>. Note that Vertex AI is available in only these regions.</p> <p>Scenario 2.1: Configuration to upload logs to Vertex AI Tensorboard</p> <pre><code>run_name: \"test-run\"\nuse_vertex_tensorboard: True\nvertex_tensorboard_project: \"test-project\" # or vertex_tensorboard_project: \"\"\nvertex_tensorboard_location: \"us-central1\"\n</code></pre> <p>The above configuration will try to create a Vertex AI Tensorboard instance named <code>test-project-tb-instance</code> and a Vertex AI Experiment named <code>test-run</code> in the <code>us-central1</code> region of <code>test-project</code>. If you set <code>vertex_tensorboard_project=\"\"</code>, then the default project (<code>gcloud config get project</code>) set on the VM will be used to create the Vertex AI resources. It will only create these resources if they do not already exist. Also, the logs in <code>config.tensorboard_dir</code> will be uploaded to <code>test-project-tb-instance</code> Tensorboard instance and <code>test-run</code> Experiment in Vertex AI.</p> <p>Scenario 2.2: Configuration to not upload logs to Vertex AI Tensorboard</p> <p>The following configuration will not upload any log data collected in <code>config.tensorboard_dir</code> to Tensorboard in Vertex AI.</p> <pre><code>use_vertex_tensorboard: False\nvertex_tensorboard_project: \"\"\nvertex_tensorboard_location: \"\"\n</code></pre>"},{"location":"getting_started/steps_model/","title":"Steps to build a Model","text":"<p> Fig1: Stages of LLM Model Development from pre-training to fine tuning and finally serving a model.</p> <p>Model building starts with Pre-training a base model architecture. Pre-training is the process where you take a model architecture, which starts with random weights and train with a very large corpus in the scale of trillions of tokens.  E.g. Google\u2019s Gemma models were pre-trained on 6 Trillion tokens; LLama 3 was trained with 15 Trillion tokens</p> <p>Post the pre-training most model producers will publish a checkpoint of the weights of the model. The corpus used for pre-training these models are usually a large public corpus like Common Crawl, public code bases, books etc.</p> <p>Though these may be a great way to answer very general questions or prompts, they usually fail on very domain specific questions and answers like Medical and Life Sciences,  Engineering,   etc.</p> <p>Customers and enterprises usually like to continue training a pre-trained model or performing a full fine tuning of the models using their own datasets. These datasets are usually in billions of tokens.  This allows better prompt understanding when questions are asked on keywords and terms specific to their model or domain specific question.</p> <p>Post a Full Fine Tuning, most models go through a process of Instruction Fine Tuning(PEFT/LoRA), Supervised Fine Tuning and RLHF to improve the model quality and follow prompt answers better.</p> <p>PEFT/Lora, Supervised Finetuning are less expensive operations compared to full fine tuning. </p>"},{"location":"reference/code_organization/","title":"MaxText Code Organization","text":"<p>Maxtext is purely written in JAX and python. Below are some folders and files that show a high-level organization of the code and some key files.</p> File/Folder Description <code>configs</code> Folder contains all the config file, including model configs (llama2, mistral etc) , and pre-optimized configs for different model size on different TPUs <code>input_pipelines</code> Input training data related code <code>layers</code> Model layer implementation <code>end_to_end</code> Example scripts to run Maxtext <code>Maxtext/train.py</code> The main training script you will run directly <code>Maxtext/config/base.yaml</code> The base configuration file containing all the related info: checkpointing, model arch, sharding schema, data input, learning rate, profile, compilation, decode <code>Maxtext/decode.py</code> This is a script to run offline inference with a sample prompt <code>setup.sh</code> Bash script used to install all needed library dependencies."},{"location":"reference/config_options/","title":"Configuration options","text":""}]}
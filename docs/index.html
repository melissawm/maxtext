<div class="doc-body">
    <section class="hero">
        <img src="./_static/maxtext.png" class="hero-image">
        <section class="hero-text">
            <div>
                <span><h1>MaxText</h1></span>
                <h3>High performance, highly scalable, open-source LLM library and reference implementation written in pure Python/JAX and targeting Google Cloud TPUs and GPUs for training.</h3>
                <div class="hero-cta">
                <a class="button button-primary" href="./tutorials.html">Get started</a>
                </div>
            </div>
        </section>
    </section>
    <section class="three-up">
        <div>
            <h3>High-performance</h3>
            <p>MaxText achieves high Model FLOPs Utilization (MFU) and tokens/second from single host to very large clusters while staying simple and largely "optimization-free" thanks to the power of JAX and the XLA compiler.</p>
        </div>
        <div>
            <h3>Pre-training</h3>
            <p>MaxText provides opinionated implementations for how to achieve optimal performance across a wide variety of dimensions like sharding, quantization, and checkpointing.</p>
        </div>
        <div>
            <h3>Post-training</h3>
            <p>MaxText provides a scalable framework using Tunix</p>
        </div>
    </section>
    <section class="banner">
        <h3>JAX AI Stack</h3>
        <p>The JAX AI Stack is a curated collection of libraries that researchers and engineers, both inside and outside of Google, have found useful for implementing and deploying the models behind generative AI tools like Imagen, Gemini, and more.</p>
        <ul>
            <li><a href="http://jax.dev/">JAX</a> - core array operations and program transformations</li>
            <li><a href="https://flax.readthedocs.io/en/latest/">Flax</a> - For building neural networks</li>
            <li><a href="https://orbax.readthedocs.io/en/latest/">Orbax</a> - For checkpointing and persistence utilities</li>
            <li><a href="https://optax.readthedocs.io/en/latest/">Optax</a> - For gradient processing and optimization</li>
            <li><a href="https://tunix.readthedocs.io/en/latest/">Tunix</a> - A JAX-native LLM Post-Training Library</li>
            <li><a href="https://github.com/jax-ml/ml_dtypes">ml_dtypes</a> - NumPy dtype extensions for machine learning.</li>
            <li>Optional data loading libraries (<a href="https://google-grain.readthedocs.io/en/latest/">Grain</a> or <a href="https://www.tensorflow.org/guide/data">tf.data</a>)</li>
        </ul>
    </section>
    <section class="text-body">
        <h3>Latest News</h3>
        <p></p>
            <ul>
                <li> <i>September 24, 2025</i> The GPT-OSS family of models (20B, 120B) is now supported.</li>
                <li> <i>September 5, 2025</i> MaxText has moved to an `src` layout. <a href="https://github.com/AI-Hypercomputer/maxtext/blob/main/RESTRUCTURE.md">Read more</a></li>
                <li> <i>August 13, 2025</i> The Qwen3 2507 MoE family of models is now supported: MoEs: 235B Thinking & 280B Coder as well as existing dense models: 0.6B, 4B, 8B, 14B, and 32B.</li>
                <li> <i>July 27, 2025</i> Updated TFLOPS/s calculation (<a href="https://github.com/AI-Hypercomputer/maxtext/pull/1988">PR</a>) to account for causal attention, dividing the attention flops in half. Accounted for sliding window and chunked attention reduced attention flops in <a href="https://github.com/AI-Hypercomputer/maxtext/pull/2009">PR</a> and <a href="https://github.com/AI-Hypercomputer/maxtext/pull/2030">PR</a>. Changes impact large sequence configs, as explained in <a href="https://github.com/AI-Hypercomputer/maxtext/blob/main/docs/guides/performance_metrics.md">this doc</a></li>
                <li> <i>July 16, 2025</i> We will be restructuring the MaxText repository for improved organization and clarity. Please review the <a href="https://github.com/AI-Hypercomputer/maxtext/blob/main/RESTRUCTURE.md">proposed structure</a> and provide feedback.</li>
                <li> <i>July 11, 2025</i> Multi-Token Prediction (MTP) training support! Adds an auxiliary loss based on predicting multiple future tokens, inspired by <a href="https://arxiv.org/html/2412.19437v1">DeepSeek-V3 paper</a>, to enhance training efficiency.</li>
                <li> <i>June 25, 2025</i> DeepSeek R1-0528 variant is now supported.</li>
                <li> <i>April 24, 2025</i> Llama 4 Maverick models are now supported.</li>
            </ul>
        </p>
    </section>
</div>
